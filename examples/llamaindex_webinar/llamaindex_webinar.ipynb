{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "074e394b",
   "metadata": {},
   "source": [
    "# Llama Index x Tonic Validate Webinar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8944d89",
   "metadata": {},
   "source": [
    "## Setting Up Llama Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe5b069",
   "metadata": {},
   "source": [
    "### Setting up local embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006762a9",
   "metadata": {},
   "source": [
    "`BAAI/bge-small-en-v1.5` is a local embedding model which replaces the default OpenAI embedding model. This model is known for being focused on RAG and has good performance for llama-index. By using a local model, we can avoid the need to send our private data to a remote server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab805b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Set the default embedding model to BAAI/bge-small-en-v1.5\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f28cd5",
   "metadata": {},
   "source": [
    "### Setting up Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e2fbb1",
   "metadata": {},
   "source": [
    "Ollama is a tool for running local models easily on your computer. To use Ollama with LlamaIndex, we must set the default LLM to use Ollama. We are using Llama2 70b for the model we are running on Ollama. We chose Llama2 70b because of it's ability to follow instructions better than smaller models. However, due to the model's size we are running the model on a separate server with 4 A10G GPUs. We also raised the amount of time it takes for the LLM request to time out due to how long the 70b version of Llama2 takes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefebf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "import os\n",
    "\n",
    "Settings.llm = Ollama(model=\"llama2:70b-chat\", base_url=os.getenv(\"OLLAMA_URL\"), request_timeout=180.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0992a6",
   "metadata": {},
   "source": [
    "### Setting up Llama Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d1769e",
   "metadata": {},
   "source": [
    "First, we will load our data for RAG into Llama Index. For our data, we will be using a collection of Paul Grahams essays and we will be asking questions about his essays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be7b5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"../paul_graham_essays\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb58a9ea",
   "metadata": {},
   "source": [
    "Now we can set up our query engine and write a simple function to output our results from the query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14c362a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EthanPhilpott\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Response\n",
    "from tonic_validate import CallbackLLMResponse\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Gets the response from llama index in a format Tonic Validate can understand\n",
    "def get_llama_response(prompt) -> CallbackLLMResponse:\n",
    "    response = query_engine.query(prompt)\n",
    "    # Check response is of type Response\n",
    "    if not isinstance(response, Response):\n",
    "        raise ValueError(f\"Expected Response, got {type(response)}\")\n",
    "    \n",
    "    # Get the response and context from the Llama index\n",
    "    context = [x.text for x in response.source_nodes]\n",
    "    answer = response.response\n",
    "    if answer is None:\n",
    "        raise ValueError(\"No response from Llama\")\n",
    "    \n",
    "    return {\n",
    "        \"llm_answer\": answer,\n",
    "        \"llm_context_list\": context\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f08b09",
   "metadata": {},
   "source": [
    "### Asking questions to Llama Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c472cf",
   "metadata": {},
   "source": [
    "Now that we have Llama Index set up, we can load our questions to ask Llama Index about the Paul Graham essays. In the following code, we will just load our 10 questions from a json file with the questions. We also have reference answers for each question which represents the ideal answer to the question. For instance, if you have a question \"What is the capital of France\" then the reference answer would be \"Paris\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c945cbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "qa_pairs = []\n",
    "with open(\"../question_and_answer_list.json\", \"r\") as qa_file:\n",
    "    qa_pairs = json.load(qa_file)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2c5e32",
   "metadata": {},
   "source": [
    "Let's view the questions and answers in the json file we just loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b6bca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_qa_pair(qa_pair):\n",
    "    print(f\"Question: {qa_pair['question']}\")\n",
    "    print(f\"Answer: {qa_pair['answer']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c6dbb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What makes Sam Altman a good founder?\n",
      "Answer: He has a great force of will.\n",
      "\n",
      "Question: When was the essay \"Five Founders\" written?\n",
      "Answer: April 2009\n",
      "\n",
      "Question: When does the most dramatic growth happen for a startup?\n",
      "Answer: When the startup only has three or four people.\n",
      "\n",
      "Question: What is the problem with business culture versus start up culture with respect to productivity?\n",
      "Answer: In business culture, energy is expended on outward appearance to the detriment of productivity, while in startup culture there is no value of appearance it's all about productivity.\n",
      "\n",
      "Question: What's the single biggest thing the government could do to increase the number of startups in this country?\n",
      "Answer: Establish a new class of visa for startup founders.\n",
      "\n",
      "Question: How could one create a rigorous government definition of what a startup is to categorize whether a business is a startup?\n",
      "Answer: One could define a startup as a company that has received investment by recognized startup investors. An accreditation procedure would be determined to establish recognized startup investors.\n",
      "\n",
      "Question: Why is frienship a good quality of founders?\n",
      "Answer: Friendship is a good quality of founders because building a startup is tough and without a strong friendship building the startup will tear the founders apart.\n",
      "\n",
      "Question: Why is determination the most important quality in startup founders?\n",
      "Answer: Because when building a startup you're going to hit a lot of obstacles, and determination will prevent one from getting demoralized by the constant obstacles.\n",
      "\n",
      "Question: For startups, what does board control mean in practice?\n",
      "Answer: During board meetings, matters are decided in the discussion preceding the vote, not in the vote itself, which is usually unanimous. If opinion is divided in such discussions, the side that knows it would lose in a vote tends to be less insistent. This is what board control means in practice.\n",
      "\n",
      "Question: What's in the way of founders keeping board control after a series A?\n",
      "Answer: The perception that founders keeping board control after a series A being a concession a VC has made when negotiating with startups. VCs do not want to look like they've lost a negotiation. If founders keeping control after a series A is the norm, then it happening will not look like the VCs lost the negotiation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for qa_pair in qa_pairs:\n",
    "    print_qa_pair(qa_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79df11d",
   "metadata": {},
   "source": [
    "Let's take one of the questions we loaded and ask it to Llama Index to see the response quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25eedb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What makes Sam Altman a good founder?\n",
      "Answer: He has a great force of will.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_qa = qa_pairs[0]\n",
    "print_qa_pair(example_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94b57275",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_llama_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mget_llama_response\u001b[49m(example_qa[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_llama_response' is not defined"
     ]
    }
   ],
   "source": [
    "get_llama_response(example_qa[\"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ce380a",
   "metadata": {},
   "source": [
    "## Using Tonic Validate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904df61d",
   "metadata": {},
   "source": [
    "Now let's set up Tonic Validate to score the questions. First, we will set up a benchmark in Tonic Validate. A benchmark is just a list of questions and reference answers that we will use to score the response quality. We will use the QA pairs we loaded earlier for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f561d8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tonic_validate import Benchmark\n",
    "question_list = [qa_pair['question'] for qa_pair in qa_pairs]\n",
    "answer_list = [qa_pair['answer'] for qa_pair in qa_pairs]\n",
    "\n",
    "benchmark = Benchmark(questions=question_list, answers=answer_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a255c6",
   "metadata": {},
   "source": [
    "Now we can run through the questions and score the response quality with Tonic Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "262ed013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the responses into an array for scoring\n",
    "from tonic_validate import LLMResponse\n",
    "\n",
    "\n",
    "responses = []\n",
    "for item in benchmark:\n",
    "    rag_response = get_llama_response(item.question)\n",
    "    llm_response = LLMResponse(\n",
    "        llm_answer=rag_response[\"llm_answer\"],\n",
    "        llm_context_list=rag_response[\"llm_context_list\"],\n",
    "        benchmark_item=item\n",
    "    )\n",
    "    responses.append(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a9b1901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the responses to a pickle file\n",
    "import os\n",
    "import pickle\n",
    "# Check if pickle file exists\n",
    "file_name = \"llm_responses.pkl\"\n",
    "if not os.path.exists(file_name):\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        pickle.dump(responses, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f13c6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the responses from the pickle file\n",
    "import pickle\n",
    "with open(\"llm_responses.pkl\", \"rb\") as f:\n",
    "    responses = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b585568",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring responses: 100%|██████████| 10/10 [12:05<00:00, 72.54s/it]\n"
     ]
    }
   ],
   "source": [
    "from tonic_validate import ValidateScorer\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_BASE_URL\"] = \"http://54.235.13.59:8080/v1\"\n",
    "scorer = ValidateScorer(model_evaluator=\"llama2:70b-chat\")\n",
    "response_scores = scorer.score_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0de9cc5-479d-46a2-a8eb-821436ac7e8d",
   "metadata": {},
   "source": [
    "Let's view the results in a dataframe to see the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f07d61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_scores.to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5432a16-53b3-4f05-bba0-066a6fcf7d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def make_scores_df(response_scores):\n",
    "    scores_df = {\n",
    "        \"question\": [],\n",
    "        \"reference_answer\": [],\n",
    "        \"llm_answer\": [],\n",
    "        \"retrieved_context\": []\n",
    "    }\n",
    "    for score_name in response_scores.overall_scores:\n",
    "        scores_df[score_name] = []\n",
    "    for data in response_scores.run_data:\n",
    "        scores_df[\"question\"].append(data.reference_question)\n",
    "        scores_df[\"reference_answer\"].append(data.reference_answer)\n",
    "        scores_df[\"llm_answer\"].append(data.llm_answer)\n",
    "        scores_df[\"retrieved_context\"].append(data.llm_context)\n",
    "        for score_name, score in data.scores.items():\n",
    "            scores_df[score_name].append(score)\n",
    "    return pd.DataFrame(scores_df)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "319b1c47-c816-4d77-bd33-1606f733c0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = make_scores_df(response_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76d0cec9-2d08-4b7b-a4f0-9ca865a284a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>llm_answer</th>\n",
       "      <th>retrieved_context</th>\n",
       "      <th>answer_similarity</th>\n",
       "      <th>augmentation_precision</th>\n",
       "      <th>answer_consistency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What makes Sam Altman a good founder?</td>\n",
       "      <td>He has a great force of will.</td>\n",
       "      <td>According to Paul Graham's essay, Sam Altman i...</td>\n",
       "      <td>[You can't plan when you start a startup how l...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When was the essay \"Five Founders\" written?</td>\n",
       "      <td>April 2009</td>\n",
       "      <td>The essay \"Five Founders\" was written in April...</td>\n",
       "      <td>[Written by Paul Graham\\r\\n\\r\\nFive Founders\\r...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When does the most dramatic growth happen for ...</td>\n",
       "      <td>When the startup only has three or four people.</td>\n",
       "      <td>According to the provided text, the most drama...</td>\n",
       "      <td>[Written by Paul Graham\\r\\n\\r\\nStartup = Growt...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the problem with business culture vers...</td>\n",
       "      <td>In business culture, energy is expended on out...</td>\n",
       "      <td>According to the provided text, the issue with...</td>\n",
       "      <td>[Written by Paul Graham\\r\\n\\r\\nLearning from F...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What's the single biggest thing the government...</td>\n",
       "      <td>Establish a new class of visa for startup foun...</td>\n",
       "      <td>According to the provided text, the single big...</td>\n",
       "      <td>[Written by Paul Graham\\r\\n\\r\\nThe Founder Vis...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0              What makes Sam Altman a good founder?   \n",
       "1        When was the essay \"Five Founders\" written?   \n",
       "2  When does the most dramatic growth happen for ...   \n",
       "3  What is the problem with business culture vers...   \n",
       "4  What's the single biggest thing the government...   \n",
       "\n",
       "                                    reference_answer  \\\n",
       "0                      He has a great force of will.   \n",
       "1                                         April 2009   \n",
       "2    When the startup only has three or four people.   \n",
       "3  In business culture, energy is expended on out...   \n",
       "4  Establish a new class of visa for startup foun...   \n",
       "\n",
       "                                          llm_answer  \\\n",
       "0  According to Paul Graham's essay, Sam Altman i...   \n",
       "1  The essay \"Five Founders\" was written in April...   \n",
       "2  According to the provided text, the most drama...   \n",
       "3  According to the provided text, the issue with...   \n",
       "4  According to the provided text, the single big...   \n",
       "\n",
       "                                   retrieved_context  answer_similarity  \\\n",
       "0  [You can't plan when you start a startup how l...                4.0   \n",
       "1  [Written by Paul Graham\\r\\n\\r\\nFive Founders\\r...                5.0   \n",
       "2  [Written by Paul Graham\\r\\n\\r\\nStartup = Growt...                2.0   \n",
       "3  [Written by Paul Graham\\r\\n\\r\\nLearning from F...                5.0   \n",
       "4  [Written by Paul Graham\\r\\n\\r\\nThe Founder Vis...                5.0   \n",
       "\n",
       "   augmentation_precision  answer_consistency  \n",
       "0                     1.0            0.666667  \n",
       "1                     1.0            1.000000  \n",
       "2                     1.0            0.500000  \n",
       "3                     1.0            0.800000  \n",
       "4                     1.0            1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e21dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tonic_validate import ValidateApi\n",
    "\n",
    "validate_api = ValidateApi()\n",
    "validate_api.upload_run(\"project-id\", response_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
