{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0o-4pVNHKDt"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "g-z_1VNFHKDv"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from openai import OpenAI\n",
        "from tonic_validate import ValidateScorer, Benchmark, BenchmarkItem, LLMResponse, BenchmarkItem, Run\n",
        "from tonic_validate.metrics import AnswerSimilarityMetric\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "\n",
        "with open(\"qa_pairs.json\", \"r\") as f:\n",
        "    qa_pairs = json.load(f)\n",
        "\n",
        "# for testing\n",
        "qa_pairs = qa_pairs\n",
        "\n",
        "benchmark = Benchmark(\n",
        "    questions=[x[\"question\"] for x in qa_pairs],\n",
        "    answers=[x[\"answer\"] for x in qa_pairs]\n",
        ")\n",
        "\n",
        "def run_to_dataframe(run: Run) -> pd.DataFrame:\n",
        "    return pd.DataFrame(\n",
        "        {\n",
        "            \"reference_question\": [x.reference_question for x in run.run_data],\n",
        "            \"reference_answer\": [x.reference_answer for x in run.run_data],\n",
        "            \"llm_answer\": [x.llm_answer for x in run.run_data],\n",
        "            \"llm_context\": [json.dumps(x.llm_context) for x in run.run_data],\n",
        "            \"answer_similarity\": [x.scores[\"answer_similarity\"] for x in run.run_data]\n",
        "        }\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcsKPLRTHKDv"
      },
      "source": [
        "### CustomGPT Conversation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHxCvqIzHKDw"
      },
      "source": [
        "#### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Sntzr9vUHKDw"
      },
      "outputs": [],
      "source": [
        "from customgpt_client import CustomGPT\n",
        "from customgpt_client.types import File\n",
        "import uuid\n",
        "CustomGPT.api_key = userdata.get('CUSTOMGPT_API_KEY')\n",
        "\n",
        "def get_customgpt_rag_response(benchmarkItem: BenchmarkItem, project):\n",
        "    prompt = benchmarkItem.question\n",
        "    conversation_id = uuid.uuid4()\n",
        "    max_retries = 3\n",
        "    run_count = 0\n",
        "\n",
        "    while run_count < max_retries:\n",
        "        try:\n",
        "            response = CustomGPT.Conversation.send(project_id=project.id, session_id=conversation_id, prompt=prompt)\n",
        "            if response.status_code == 200:\n",
        "                openai_response = response.parsed.data.openai_response\n",
        "            else:\n",
        "                raise Exception(f\"Failed in generating CustomGPT Response::{response.status_code}\")\n",
        "            return openai_response\n",
        "        except Exception as e:\n",
        "            run_count += 1\n",
        "            time.sleep(2)\n",
        "\n",
        "\n",
        "def setup_project():\n",
        "    file_ids = []\n",
        "    project = CustomGPT.Project.create(project_name=\"Rag Evaluation\")\n",
        "    if project.status_code == 201:\n",
        "        project_id = project.parsed.data.id\n",
        "\n",
        "        source = CustomGPT.Source.create(project_id=project_id, file=File(payload=open(\"all_essays_in_single_file.txt\", 'rb'), file_name=f\"Paul Graham Essay\"))\n",
        "\n",
        "        is_chat_active = 0\n",
        "        # Check to make sure at least one page is indexed\n",
        "        while not is_chat_active:\n",
        "            response_project = CustomGPT.Project.get(project_id=project_id)\n",
        "            json_project = response_project.parsed\n",
        "            is_chat_active = json_project.data.is_chat_active\n",
        "            time.sleep(5)\n",
        "\n",
        "        return project.parsed.data\n",
        "    else:\n",
        "        raise Exception(\"CustomGPT Project Creation Failed\")\n",
        "    pass\n",
        "\n",
        "project = setup_project()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7y3BWAlHKDw"
      },
      "source": [
        "#### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9dFrO85HKDw"
      },
      "outputs": [],
      "source": [
        "\n",
        "benchmark_item = BenchmarkItem(\n",
        "    question=\"What key components are necessary to create a technology hub according to Paul Graham?\",\n",
        "    answer=\"\"\n",
        ")\n",
        "\n",
        "print(get_customgpt_rag_response(benchmark_item, project))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU3dDuRCHKDx"
      },
      "source": [
        "##### Run through all the questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HS_HHmByHKDx"
      },
      "outputs": [],
      "source": [
        "raw_customgpt_responses = []\n",
        "for x in tqdm(benchmark.items):\n",
        "    raw_customgpt_responses.append(get_customgpt_rag_response(x, project))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "oosHPNIeHKDy"
      },
      "outputs": [],
      "source": [
        "customgpt_responses = [\n",
        "    LLMResponse(\n",
        "        llm_answer=r, llm_context_list=[], benchmark_item=bi\n",
        "    ) for r, bi in zip(raw_customgpt_responses, benchmark.items)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "hKtK4EY9HKDy"
      },
      "outputs": [],
      "source": [
        "scorer = ValidateScorer([AnswerSimilarityMetric()])\n",
        "customgpt_run = scorer.score_run(customgpt_responses, parallelism=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "jaPFJkngHKDy"
      },
      "outputs": [],
      "source": [
        "customgpt_run_df = run_to_dataframe(customgpt_run)\n",
        "customgpt_run_df.to_csv(\"customgpt_run.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ouo4BP1HKDz"
      },
      "outputs": [],
      "source": [
        "customgpt_answer_similarity_scores = pd.Series([x.scores[\"answer_similarity\"] for x in customgpt_run.run_data])\n",
        "category_counts = customgpt_answer_similarity_scores.value_counts()\n",
        "plt.bar(category_counts.index, category_counts.values)\n",
        "\n",
        "plt.title('Distribution of scores for CustomGPT')\n",
        "plt.xlabel('Score')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.bar(category_counts.index, category_counts.values, color='#A679C8')\n",
        "\n",
        "# Remove all scores except whole numbers\n",
        "plt.xticks(range(0, 6, 1))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28jsFJCXHKDz"
      },
      "source": [
        "### OpenAI Rag Assistant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pQGqQRmHKDz"
      },
      "source": [
        "#### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "0y5Rs0riHKD0"
      },
      "outputs": [],
      "source": [
        "def get_openai_rag_response(benchmarkItem: BenchmarkItem, assistant):\n",
        "    prompt = benchmarkItem.question\n",
        "    thread = client.beta.threads.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "    run = client.beta.threads.runs.create(\n",
        "        thread_id=thread.id,\n",
        "        assistant_id=assistant.id,\n",
        "    )\n",
        "    max_retries = 10\n",
        "    base = 0.1\n",
        "    num_retries = 0\n",
        "    try:\n",
        "        while max_retries > 0:\n",
        "            if max_retries == 0:\n",
        "                client.beta.threads.delete(thread.id)\n",
        "                raise Exception(\"Max tries exceeded\")\n",
        "            messages = client.beta.threads.messages.list(\n",
        "                thread_id=thread.id,\n",
        "            )\n",
        "            if len(messages.data)>0 and len(messages.data[0].content)>0:\n",
        "                response_message = messages.data[0].content[0].text.value\n",
        "                if response_message != prompt and response_message.strip():\n",
        "                    annotations = messages.data[0].content[0].text.annotations\n",
        "                    quotes = [x.file_citation.quote for x in annotations if x.file_citation]\n",
        "                    client.beta.threads.delete(thread.id)\n",
        "                    return response_message\n",
        "            time.sleep(base * (2 ** num_retries))\n",
        "            num_retries += 1\n",
        "            max_retries -= 1\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        client.beta.threads.delete(thread.id)\n",
        "        raise e\n",
        "\n",
        "#openai rag only supports a max of 20 files. So we just combine all essays into single file\n",
        "def upload_essay():\n",
        "    file_ids=[]\n",
        "    with open(\"all_essays_in_single_file.txt\", 'rb') as essay_file:\n",
        "        file = client.files.create(\n",
        "            file=essay_file,\n",
        "            purpose='assistants'\n",
        "        )\n",
        "        file_ids.append(file.id)\n",
        "    return file_ids\n",
        "\n",
        "def create_assistant(file_ids):\n",
        "    return client.beta.assistants.create(\n",
        "        name=f\"OpenAI Rag Test {len(file_ids)} Files\",\n",
        "        instructions=(\n",
        "            \"You are a chatbot that answers questions about Paul Graham's essays. \"\n",
        "            \"Use your knowledge base to best respond to questions. \"\n",
        "            \"NO MATTER WHAT, DO NOT PULL INFORMATION FROM EXTERNAL KNOWLEDGE. ONLY USE YOUR OWN KNOWLEDGE BASE.\"\n",
        "        ),\n",
        "        model=\"gpt-4-1106-preview\",\n",
        "        tools=[{\"type\": \"retrieval\"}],\n",
        "        file_ids=file_ids\n",
        "    )\n",
        "\n",
        "def setup_assistant():\n",
        "    file_ids = upload_essay()\n",
        "    return create_assistant(file_ids)\n",
        "\n",
        "# WARNING\n",
        "# this deletes all files associated with your openai api key.\n",
        "#def cleanup_files():\n",
        "#    for f in client.files.list():\n",
        "#        client.files.delete(f.id)\n",
        "#        time.sleep(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OppxdEXMHKD0"
      },
      "source": [
        "#### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "_htm-vGVHKD0"
      },
      "outputs": [],
      "source": [
        "client = OpenAI()\n",
        "assistant = setup_assistant()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIvH8slVHKD1"
      },
      "outputs": [],
      "source": [
        "benchmark_item = BenchmarkItem(\n",
        "    question=\"What key components are necessary to create a technology hub according to Paul Graham?\",\n",
        "    answer=\"\"\n",
        ")\n",
        "\n",
        "print(get_openai_rag_response(benchmark_item, assistant))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6831C1JHKD1"
      },
      "source": [
        "##### Run through all the questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ui6tWclTHKD1",
        "outputId": "369d0322-25d5-409a-b408-8c6e055e7383"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 55/55 [27:13<00:00, 29.70s/it]\n"
          ]
        }
      ],
      "source": [
        "raw_openai_responses = []\n",
        "for x in tqdm(benchmark.items):\n",
        "    raw_openai_responses.append(get_openai_rag_response(x, assistant))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "AQkJHWBmHKD2"
      },
      "outputs": [],
      "source": [
        "openai_responses = [\n",
        "    LLMResponse(\n",
        "        llm_answer=r, llm_context_list=[], benchmark_item=bi\n",
        "    ) for r, bi in zip(raw_openai_responses, benchmark.items)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "4hyOfpizHKD2"
      },
      "outputs": [],
      "source": [
        "scorer = ValidateScorer([AnswerSimilarityMetric()])\n",
        "openai_run = scorer.score_run(openai_responses, parallelism=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "TK-JuvgfHKD2"
      },
      "outputs": [],
      "source": [
        "openai_run_df = run_to_dataframe(openai_run)\n",
        "openai_run_df.to_csv(\"openai_run.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqz2640HHKD2"
      },
      "outputs": [],
      "source": [
        "openai_answer_similarity_scores = pd.Series([x.scores[\"answer_similarity\"] for x in openai_run.run_data])\n",
        "category_counts = openai_answer_similarity_scores.value_counts()\n",
        "plt.bar(category_counts.index, category_counts.values)\n",
        "\n",
        "plt.title('Distribution of scores for OpenAI')\n",
        "plt.xlabel('Score')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.bar(category_counts.index, category_counts.values, color='#A679C8')\n",
        "\n",
        "# Remove all scores except whole numbers\n",
        "plt.xticks(range(0, 6, 1))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0CBqhk7HKD2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "openai_run_df = pd.read_csv(\"openai_run.csv\")\n",
        "customgpt_run_df = pd.read_csv(\"customgpt_run.csv\")\n",
        "\n",
        "combined_scores = pd.DataFrame({\n",
        "    \"OpenAI\": list(openai_run_df[\"answer_similarity\"]),\n",
        "    \"CustomGPT\": list(customgpt_run_df[\"answer_similarity\"])\n",
        "})\n",
        "\n",
        "# Position of bars on x-axis\n",
        "ind = np.arange(6)\n",
        "\n",
        "# Figure size\n",
        "plt.figure(figsize=(10,5))\n",
        "\n",
        "# Width of a bar\n",
        "width = 0.3\n",
        "\n",
        "openai_counts = combined_scores[\"OpenAI\"].value_counts().reindex([0,1,2,3,4,5], fill_value=0)\n",
        "plt.bar(openai_counts.index, openai_counts.values, width, label=\"OpenAI\", color=\"#5EA7EC\")\n",
        "\n",
        "customgpt_counts = combined_scores[\"CustomGPT\"].value_counts().reindex([0,1,2,3,4,5], fill_value=0)\n",
        "plt.bar(customgpt_counts.index + width, customgpt_counts.values, width, label=\"CustomGPT\", color=\"#A679C8\")\n",
        "\n",
        "plt.title('Distribution of scores for OpenAI & CustomGPT')\n",
        "plt.xlabel('Answer Similarity Score')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "# Remove all scores except whole numbers\n",
        "plt.xticks(ind + width / 2, range(0, 6, 1))\n",
        "\n",
        "plt.legend(loc='best')\n",
        "\n",
        "# Rounding the descriptive statistics to 3 significant figures\n",
        "statistics_df = combined_scores.describe()\n",
        "# Change 50% to 50% (median)\n",
        "statistics_df = statistics_df.rename(index={'50%': 'median'})\n",
        "rounded_statistics_df = statistics_df.round(3)\n",
        "\n",
        "# Transposing the descriptive statistics table for horizontal display\n",
        "rounded_statistics_df = rounded_statistics_df.T\n",
        "\n",
        "# Adding the table below the histogram\n",
        "table = plt.table(cellText=rounded_statistics_df.values,\n",
        "                  colWidths=[0.1] * len(rounded_statistics_df.columns),\n",
        "                  rowLabels=rounded_statistics_df.index,\n",
        "                  colLabels=rounded_statistics_df.columns,\n",
        "                  cellLoc = 'center', rowLoc = 'center',\n",
        "                  loc='bottom', bbox=[0.0, -0.5, 1.0, 0.3])\n",
        "\n",
        "plt.subplots_adjust(left=0.2, bottom=0.3)\n",
        "plt.savefig(\"openai_v_customgpt.png\", facecolor='white', bbox_inches='tight')\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}