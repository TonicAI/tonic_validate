{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Quickstart\n",
    "This quickstart is based on the one from the readme. It is only to be used as a starting point / example for how to use Tonic Validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-16T15:06:53.334191Z"
    },
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving responses: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Scoring responses:   0%|          | 0/1 [00:00<?, ?it/s]c:\\Users\\EthanPhilpott\\tonic_validate\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[92m16:46:04 - LiteLLM:INFO\u001b[0m: utils.py:1133 - \u001b[92m{'model': 'gemini-1.5-pro-latest', 'messages': [{'role': 'user', 'content': 'Considering the reference answer and the new answer to the following question, on a scale of 0 to 5, where 5 means the same and 0 means not at all similar, how similar in meaning is the new answer to the reference answer? Respond with just a number and no additional text.\\nQUESTION: What is the capital of France?\\nREFERENCE ANSWER: Paris\\nNEW ANSWER: Paris\\n'}], 'optional_params': {'temperature': 0.0}, 'litellm_params': {'acompletion': True, 'api_key': None, 'force_timeout': 600, 'logger_fn': None, 'verbose': False, 'custom_llm_provider': 'gemini', 'api_base': '', 'litellm_call_id': 'ccd4a92c-cc7b-494b-a4ab-b7722f657060', 'model_alias_map': {}, 'completion_call_id': None, 'metadata': None, 'model_info': None, 'proxy_server_request': None, 'preset_cache_key': None, 'no-log': False, 'stream_response': {}}, 'start_time': datetime.datetime(2024, 4, 23, 16, 46, 3, 689491), 'stream': False, 'user': None, 'call_type': 'acompletion', 'litellm_call_id': 'ccd4a92c-cc7b-494b-a4ab-b7722f657060', 'completion_start_time': None, 'temperature': 0.0, 'input': ['Considering the reference answer and the new answer to the following question, on a scale of 0 to 5, where 5 means the same and 0 means not at all similar, how similar in meaning is the new answer to the reference answer? Respond with just a number and no additional text.\\nQUESTION: What is the capital of France?\\nREFERENCE ANSWER: Paris\\nNEW ANSWER: Paris\\n'], 'api_key': '', 'additional_args': {'complete_input_dict': {'inference_params': {'temperature': 0.0}, 'system_prompt': 'You are a helpful assistant. Respond using markdown.'}}, 'log_event_type': 'pre_api_call'}\u001b[0m\n",
      "\n",
      "\u001b[92m16:46:05 - LiteLLM:INFO\u001b[0m: utils.py:1133 - \u001b[92m{'model': 'gemini-1.5-pro-latest', 'messages': [{'role': 'user', 'content': 'Considering the following question and context, determine whether the context is relevant for answering the question. If the context is relevant for answering the question, respond with true. If the context is not relevant for answering the question, respond with false. Respond with either true or false and no additional text.\\nQUESTION: What is the capital of France?\\nCONTEXT: Paris is the capital of France.\\n'}], 'optional_params': {'temperature': 0.0}, 'litellm_params': {'acompletion': True, 'api_key': None, 'force_timeout': 600, 'logger_fn': None, 'verbose': False, 'custom_llm_provider': 'gemini', 'api_base': '', 'litellm_call_id': 'ee5dd049-a10c-46b2-88bd-55ce649ef945', 'model_alias_map': {}, 'completion_call_id': None, 'metadata': None, 'model_info': None, 'proxy_server_request': None, 'preset_cache_key': None, 'no-log': False, 'stream_response': {}}, 'start_time': datetime.datetime(2024, 4, 23, 16, 46, 5, 268917), 'stream': False, 'user': None, 'call_type': 'acompletion', 'litellm_call_id': 'ee5dd049-a10c-46b2-88bd-55ce649ef945', 'completion_start_time': None, 'temperature': 0.0, 'input': ['Considering the following question and context, determine whether the context is relevant for answering the question. If the context is relevant for answering the question, respond with true. If the context is not relevant for answering the question, respond with false. Respond with either true or false and no additional text.\\nQUESTION: What is the capital of France?\\nCONTEXT: Paris is the capital of France.\\n'], 'api_key': '', 'additional_args': {'complete_input_dict': {'inference_params': {'temperature': 0.0}, 'system_prompt': 'You are a helpful assistant. Respond using markdown.'}}, 'log_event_type': 'pre_api_call'}\u001b[0m\n",
      "\n",
      "\u001b[92m16:46:06 - LiteLLM:INFO\u001b[0m: utils.py:1133 - \u001b[92m{'model': 'gemini-1.5-pro-latest', 'messages': [{'role': 'user', 'content': 'Considering the following answer and context, determine whether the answer contains information derived from the context. If the answer contains information derived from the context, respond with true. If the answer does not contain information derived from the context, respond with false. Respond with either true or false and no additional text.\\nANSWER: Paris\\nCONTEXT: Paris is the capital of France.\\n'}], 'optional_params': {'temperature': 0.0}, 'litellm_params': {'acompletion': True, 'api_key': None, 'force_timeout': 600, 'logger_fn': None, 'verbose': False, 'custom_llm_provider': 'gemini', 'api_base': '', 'litellm_call_id': '92aa2bef-0618-4c0f-bb1b-942419e6e8d8', 'model_alias_map': {}, 'completion_call_id': None, 'metadata': None, 'model_info': None, 'proxy_server_request': None, 'preset_cache_key': None, 'no-log': False, 'stream_response': {}}, 'start_time': datetime.datetime(2024, 4, 23, 16, 46, 6, 506534), 'stream': False, 'user': None, 'call_type': 'acompletion', 'litellm_call_id': '92aa2bef-0618-4c0f-bb1b-942419e6e8d8', 'completion_start_time': None, 'temperature': 0.0, 'input': ['Considering the following answer and context, determine whether the answer contains information derived from the context. If the answer contains information derived from the context, respond with true. If the answer does not contain information derived from the context, respond with false. Respond with either true or false and no additional text.\\nANSWER: Paris\\nCONTEXT: Paris is the capital of France.\\n'], 'api_key': '', 'additional_args': {'complete_input_dict': {'inference_params': {'temperature': 0.0}, 'system_prompt': 'You are a helpful assistant. Respond using markdown.'}}, 'log_event_type': 'pre_api_call'}\u001b[0m\n",
      "\n",
      "\u001b[92m16:46:07 - LiteLLM:INFO\u001b[0m: utils.py:1133 - \u001b[92m{'model': 'gemini-1.5-pro-latest', 'messages': [{'role': 'user', 'content': \"Using a bulleted list in markdown (so each bullet is a '*'), write down the main points in the following answer to a user's query. Respond with the bulleted list and no additional text. Only use a single '*' for each bullet and do not use a '*' anywhere in your response except for the bullets.\\nANSWER: Paris\"}], 'optional_params': {'temperature': 0.0}, 'litellm_params': {'acompletion': True, 'api_key': None, 'force_timeout': 600, 'logger_fn': None, 'verbose': False, 'custom_llm_provider': 'gemini', 'api_base': '', 'litellm_call_id': 'bf5f01e2-7299-4a66-bdd1-89dcb02be3d5', 'model_alias_map': {}, 'completion_call_id': None, 'metadata': None, 'model_info': None, 'proxy_server_request': None, 'preset_cache_key': None, 'no-log': False, 'stream_response': {}}, 'start_time': datetime.datetime(2024, 4, 23, 16, 46, 7, 432792), 'stream': False, 'user': None, 'call_type': 'acompletion', 'litellm_call_id': 'bf5f01e2-7299-4a66-bdd1-89dcb02be3d5', 'completion_start_time': None, 'temperature': 0.0, 'input': [\"Using a bulleted list in markdown (so each bullet is a '*'), write down the main points in the following answer to a user's query. Respond with the bulleted list and no additional text. Only use a single '*' for each bullet and do not use a '*' anywhere in your response except for the bullets.\\nANSWER: Paris\"], 'api_key': '', 'additional_args': {'complete_input_dict': {'inference_params': {'temperature': 0.0}, 'system_prompt': 'You are a helpful assistant. Respond using markdown.'}}, 'log_event_type': 'pre_api_call'}\u001b[0m\n",
      "\n",
      "Error getting LLM response. Setting score to None. 429 Resource has been exhausted (e.g. check quota).\n",
      "Scoring responses: 100%|██████████| 1/1 [00:04<00:00,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tonic_validate import ValidateScorer, Benchmark\n",
    "import os\n",
    "\n",
    "# Function to simulate getting a response and context from your LLM\n",
    "# Replace this with your actual function call\n",
    "def get_rag_response(question):\n",
    "    return {\n",
    "        \"llm_answer\": \"Paris\",\n",
    "        \"llm_context_list\": [\"Paris is the capital of France.\"]\n",
    "    }\n",
    "benchmark = Benchmark(questions=[\"What is the capital of France?\"], answers=[\"Paris\"])\n",
    "# Score the responses for each question and answer pair\n",
    "scorer = ValidateScorer(model_evaluator='gemini/gemini-1.5-pro-latest')\n",
    "run = scorer.score(benchmark, get_rag_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(overall_scores={'answer_similarity': 5.0, 'augmentation_precision': 1.0}, run_data=[RunData(scores={'answer_similarity': 5.0, 'augmentation_precision': 1.0, 'answer_consistency': None}, reference_question='What is the capital of France?', reference_answer='Paris', llm_answer='Paris', llm_context=['Paris is the capital of France.'])], llm_evaluator='gemini/gemini-1.5-pro-latest', id=None)\n"
     ]
    }
   ],
   "source": [
    "print(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually passing in responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring responses:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[92m16:46:08 - LiteLLM:INFO\u001b[0m: utils.py:1133 - \u001b[92m{'model': 'gemini-1.5-pro-latest', 'messages': [{'role': 'user', 'content': 'Considering the reference answer and the new answer to the following question, on a scale of 0 to 5, where 5 means the same and 0 means not at all similar, how similar in meaning is the new answer to the reference answer? Respond with just a number and no additional text.\\nQUESTION: What is the capital of France?\\nREFERENCE ANSWER: Paris\\nNEW ANSWER: Paris\\n'}], 'optional_params': {'temperature': 0.0}, 'litellm_params': {'acompletion': True, 'api_key': None, 'force_timeout': 600, 'logger_fn': None, 'verbose': False, 'custom_llm_provider': 'gemini', 'api_base': '', 'litellm_call_id': 'fda354fd-64e5-4056-90cc-bcbe61dd810a', 'model_alias_map': {}, 'completion_call_id': None, 'metadata': None, 'model_info': None, 'proxy_server_request': None, 'preset_cache_key': None, 'no-log': False, 'stream_response': {}}, 'start_time': datetime.datetime(2024, 4, 23, 16, 46, 8, 884562), 'stream': False, 'user': None, 'call_type': 'acompletion', 'litellm_call_id': 'fda354fd-64e5-4056-90cc-bcbe61dd810a', 'completion_start_time': None, 'temperature': 0.0, 'input': ['Considering the reference answer and the new answer to the following question, on a scale of 0 to 5, where 5 means the same and 0 means not at all similar, how similar in meaning is the new answer to the reference answer? Respond with just a number and no additional text.\\nQUESTION: What is the capital of France?\\nREFERENCE ANSWER: Paris\\nNEW ANSWER: Paris\\n'], 'api_key': '', 'additional_args': {'complete_input_dict': {'inference_params': {'temperature': 0.0}, 'system_prompt': 'You are a helpful assistant. Respond using markdown.'}}, 'log_event_type': 'pre_api_call'}\u001b[0m\n",
      "\n",
      "Error getting LLM response. Setting score to None. 429 Resource has been exhausted (e.g. check quota).\n",
      "\u001b[92m16:46:09 - LiteLLM:INFO\u001b[0m: utils.py:1133 - \u001b[92m{'model': 'gemini-1.5-pro-latest', 'messages': [{'role': 'user', 'content': 'Considering the following question and context, determine whether the context is relevant for answering the question. If the context is relevant for answering the question, respond with true. If the context is not relevant for answering the question, respond with false. Respond with either true or false and no additional text.\\nQUESTION: What is the capital of France?\\nCONTEXT: Paris is the capital of France.\\n'}], 'optional_params': {'temperature': 0.0}, 'litellm_params': {'acompletion': True, 'api_key': None, 'force_timeout': 600, 'logger_fn': None, 'verbose': False, 'custom_llm_provider': 'gemini', 'api_base': '', 'litellm_call_id': '8273b623-8af1-4f33-8797-c1b39c62381a', 'model_alias_map': {}, 'completion_call_id': None, 'metadata': None, 'model_info': None, 'proxy_server_request': None, 'preset_cache_key': None, 'no-log': False, 'stream_response': {}}, 'start_time': datetime.datetime(2024, 4, 23, 16, 46, 9, 56550), 'stream': False, 'user': None, 'call_type': 'acompletion', 'litellm_call_id': '8273b623-8af1-4f33-8797-c1b39c62381a', 'completion_start_time': None, 'temperature': 0.0, 'input': ['Considering the following question and context, determine whether the context is relevant for answering the question. If the context is relevant for answering the question, respond with true. If the context is not relevant for answering the question, respond with false. Respond with either true or false and no additional text.\\nQUESTION: What is the capital of France?\\nCONTEXT: Paris is the capital of France.\\n'], 'api_key': '', 'additional_args': {'complete_input_dict': {'inference_params': {'temperature': 0.0}, 'system_prompt': 'You are a helpful assistant. Respond using markdown.'}}, 'log_event_type': 'pre_api_call'}\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error getting LLM response. Setting score to None. 429 Resource has been exhausted (e.g. check quota).\n",
      "\u001b[92m16:46:09 - LiteLLM:INFO\u001b[0m: utils.py:1133 - \u001b[92m{'model': 'gemini-1.5-pro-latest', 'messages': [{'role': 'user', 'content': \"Using a bulleted list in markdown (so each bullet is a '*'), write down the main points in the following answer to a user's query. Respond with the bulleted list and no additional text. Only use a single '*' for each bullet and do not use a '*' anywhere in your response except for the bullets.\\nANSWER: Paris\"}], 'optional_params': {'temperature': 0.0}, 'litellm_params': {'acompletion': True, 'api_key': None, 'force_timeout': 600, 'logger_fn': None, 'verbose': False, 'custom_llm_provider': 'gemini', 'api_base': '', 'litellm_call_id': '552dd2ca-944c-48c4-a122-219df1411a4e', 'model_alias_map': {}, 'completion_call_id': None, 'metadata': None, 'model_info': None, 'proxy_server_request': None, 'preset_cache_key': None, 'no-log': False, 'stream_response': {}}, 'start_time': datetime.datetime(2024, 4, 23, 16, 46, 9, 264333), 'stream': False, 'user': None, 'call_type': 'acompletion', 'litellm_call_id': '552dd2ca-944c-48c4-a122-219df1411a4e', 'completion_start_time': None, 'temperature': 0.0, 'input': [\"Using a bulleted list in markdown (so each bullet is a '*'), write down the main points in the following answer to a user's query. Respond with the bulleted list and no additional text. Only use a single '*' for each bullet and do not use a '*' anywhere in your response except for the bullets.\\nANSWER: Paris\"], 'api_key': '', 'additional_args': {'complete_input_dict': {'inference_params': {'temperature': 0.0}, 'system_prompt': 'You are a helpful assistant. Respond using markdown.'}}, 'log_event_type': 'pre_api_call'}\u001b[0m\n",
      "\n",
      "Error getting LLM response. Setting score to None. 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring responses: 100%|██████████| 1/1 [00:00<00:00,  1.77it/s]\n"
     ]
    }
   ],
   "source": [
    "from tonic_validate import ValidateScorer, Benchmark, LLMResponse\n",
    "\n",
    "# Create a list of questions (required) and answers (optional) for scoring the LLM's performance\n",
    "benchmark = Benchmark(\n",
    "    questions=[\"What is the capital of France?\"],\n",
    "    answers=[\"Paris\"]\n",
    ")\n",
    "\n",
    "# Function to simulate getting a response and context from your LLM\n",
    "# Replace this with your actual function call\n",
    "def get_rag_response(question):\n",
    "    return {\n",
    "        \"llm_answer\": \"Paris\",\n",
    "        \"llm_context_list\": [\"Paris is the capital of France.\"]\n",
    "    }\n",
    "\n",
    "# Save the responses into an array for scoring\n",
    "responses = []\n",
    "for item in benchmark:\n",
    "    rag_response = get_rag_response(item.question)\n",
    "    llm_response = LLMResponse(\n",
    "        llm_answer=rag_response[\"llm_answer\"],\n",
    "        llm_context_list=rag_response[\"llm_context_list\"],\n",
    "        benchmark_item=item\n",
    "    )\n",
    "    responses.append(llm_response)\n",
    "\n",
    "# Score the responses\n",
    "scorer = ValidateScorer(model_evaluator='gemini/gemini-1.5-pro-latest')\n",
    "run = scorer.score_responses(responses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
